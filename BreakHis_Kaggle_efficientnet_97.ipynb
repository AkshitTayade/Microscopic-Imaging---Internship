{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BreakHis Cancer Images Classification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/5/50/Histopathology_of_basal-like_breast_cancer.jpg)","metadata":{}},{"cell_type":"markdown","source":"## Problem Introduction:\nThis notebook is concerned with classifying images using the **BreakHis** (Breast Cancer Histopathological Image Dataset). Breast cancer is a disease seen mainly in women and is seen as a major cause of death among women. In the year **2018**, the total deaths due to breast cancer in women was seen to be **627,000** out of **2.1 million** cases which were diagnosed. **Invasive Ductual Carcinoma (IDC)** in diagnosing breast cancer, since its subsequent digitalization is more feasible due to advancements in slide scanning technology, as well as the reduction of storage load cost in recent years. The digitialized approcches in deep learning has aided a lot in diagnosing and controlling breast cancer, with power to pre-identify the disease via deep learning methods. ","metadata":{}},{"cell_type":"markdown","source":"## Dataset Information \nThe breast cancer histopathological image dataset (BreakHis) contains **9109** microscopic images of breast tumor tissues collected from **82** patients, regarded as malignant or benign. The tissues are magnigfied at different scaling factors (**40X**, **100X**, **200X**, **400X**). In this dataset, it contains **2480** malignant and **5429** benign tumors. The images of tissues are taken to be **700X460 pixels**, **3-channel RGB**, **8** bit depth in each format, and in PNG. It is believed that this dataset can become a benchmark for future classifications of breast cancer classification.        ","metadata":{}},{"cell_type":"markdown","source":"## Import Python Libraries\nFirst, we will import the necessary libraries for our notebook.","metadata":{}},{"cell_type":"code","source":"# importing libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, time, tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import *\nimport tensorflow as tf \nfrom functools import partial\n# import keras\nfrom tensorflow.keras import layers\nimport albumentations as A\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.callbacks import CSVLogger\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:53:35.793962Z","iopub.execute_input":"2022-02-07T14:53:35.794328Z","iopub.status.idle":"2022-02-07T14:53:42.306047Z","shell.execute_reply.started":"2022-02-07T14:53:35.794238Z","shell.execute_reply":"2022-02-07T14:53:42.305334Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## GPU Confirmation","metadata":{}},{"cell_type":"markdown","source":"Let's make sure that we have a GPU installed. ","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:53:56.289361Z","iopub.execute_input":"2022-02-07T14:53:56.289635Z","iopub.status.idle":"2022-02-07T14:53:57.003938Z","shell.execute_reply.started":"2022-02-07T14:53:56.289606Z","shell.execute_reply":"2022-02-07T14:53:57.003164Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"markdown","source":"First, we load the dara that contains multiple files, one file contains the information anout all images in a csv format, and there is another directory that contains the information about the images of breast cancer tissues.   ","metadata":{}},{"cell_type":"code","source":"image_dir = '../input/breakhis/BreaKHis_v1/'\ndata_path = '../input/breakhis/Folds.csv'\n\n# experimental API for making data pipelines\ntf.data.experimental.AUTOTUNE\n\n# defining the class names\nclass_names = ['malignant', 'benign']\nprint(class_names)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:01.905606Z","iopub.execute_input":"2022-02-07T14:54:01.905907Z","iopub.status.idle":"2022-02-07T14:54:01.912590Z","shell.execute_reply.started":"2022-02-07T14:54:01.905876Z","shell.execute_reply":"2022-02-07T14:54:01.911468Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# loading the data\ndata = pd.read_csv(data_path)\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:05.692185Z","iopub.execute_input":"2022-02-07T14:54:05.692686Z","iopub.status.idle":"2022-02-07T14:54:05.849077Z","shell.execute_reply.started":"2022-02-07T14:54:05.692649Z","shell.execute_reply":"2022-02-07T14:54:05.848440Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data['fold'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:08.381227Z","iopub.execute_input":"2022-02-07T14:54:08.381950Z","iopub.status.idle":"2022-02-07T14:54:08.393076Z","shell.execute_reply.started":"2022-02-07T14:54:08.381916Z","shell.execute_reply":"2022-02-07T14:54:08.392231Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Data Structuring\nWe will strcuture the data in csv filefor our ease and for better understanding. ","metadata":{}},{"cell_type":"code","source":"# renaming and structuring the columns for better data understanding\ndata = data.rename(columns={'filename': 'path'})\ndata['label'] = data.path.apply(lambda x: x.split('/')[3])\ndata['label_int'] = data.label.apply(lambda x: class_names.index(x))\ndata['filename'] = data.path.apply(lambda x: x.split('/')[-1])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-07T14:54:11.919544Z","iopub.execute_input":"2022-02-07T14:54:11.920538Z","iopub.status.idle":"2022-02-07T14:54:12.006568Z","shell.execute_reply.started":"2022-02-07T14:54:11.920487Z","shell.execute_reply":"2022-02-07T14:54:12.005863Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# view first n rows of strucrured data\ndata.head(6)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:14.351254Z","iopub.execute_input":"2022-02-07T14:54:14.352002Z","iopub.status.idle":"2022-02-07T14:54:14.364804Z","shell.execute_reply.started":"2022-02-07T14:54:14.351966Z","shell.execute_reply":"2022-02-07T14:54:14.364105Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:15.436423Z","iopub.execute_input":"2022-02-07T14:54:15.437222Z","iopub.status.idle":"2022-02-07T14:54:15.444149Z","shell.execute_reply.started":"2022-02-07T14:54:15.437175Z","shell.execute_reply":"2022-02-07T14:54:15.442816Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Our dataset has been restructured the way we wanted it to be. Let's perform some analysis on it tnd then we will move towards the images to perform computations on them. ","metadata":{}},{"cell_type":"markdown","source":"## Analysis on Data","metadata":{}},{"cell_type":"code","source":"# making a plot to see data distribution\n# sns.figure()\nsns.set_theme()\nsns.displot(x='label', data=data)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:17.854246Z","iopub.execute_input":"2022-02-07T14:54:17.855065Z","iopub.status.idle":"2022-02-07T14:54:18.228903Z","shell.execute_reply.started":"2022-02-07T14:54:17.855024Z","shell.execute_reply":"2022-02-07T14:54:18.228131Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The graph sows that most of the samples in our data have malignant tumors, and less have benign tumors","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=data['label'], data=data)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:20.054868Z","iopub.execute_input":"2022-02-07T14:54:20.055129Z","iopub.status.idle":"2022-02-07T14:54:20.300857Z","shell.execute_reply.started":"2022-02-07T14:54:20.055101Z","shell.execute_reply":"2022-02-07T14:54:20.300144Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Modelling for Deep Learning","metadata":{}},{"cell_type":"markdown","source":"We will model the data for our training, vaidation and testing sets. ","metadata":{}},{"cell_type":"code","source":"# sorting out training, validation and testing images\ntest_images = data.groupby(by='label').sample(3000)\ntrain_images = data.drop(test_images.index).reset_index(drop=True)\ntest_images = test_images.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:23.144462Z","iopub.execute_input":"2022-02-07T14:54:23.145161Z","iopub.status.idle":"2022-02-07T14:54:23.178489Z","shell.execute_reply.started":"2022-02-07T14:54:23.145125Z","shell.execute_reply":"2022-02-07T14:54:23.177450Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# making splits of training & validation datasets\nvalidation_images = train_images.sample(frac = 0.3)\ntrain_images = train_images.drop(validation_images.index).reset_index(drop=True)\nvalidation_images = validation_images.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:23.935843Z","iopub.execute_input":"2022-02-07T14:54:23.936468Z","iopub.status.idle":"2022-02-07T14:54:23.962112Z","shell.execute_reply.started":"2022-02-07T14:54:23.936428Z","shell.execute_reply":"2022-02-07T14:54:23.961492Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print('Total training images: % s' % str(train_images.shape[0]))\nprint('Total validation images: % s' % str(validation_images.shape[0]))\nprint('Total testing images: % s' % str(test_images.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:24.859246Z","iopub.execute_input":"2022-02-07T14:54:24.859834Z","iopub.status.idle":"2022-02-07T14:54:24.865857Z","shell.execute_reply.started":"2022-02-07T14:54:24.859797Z","shell.execute_reply":"2022-02-07T14:54:24.865151Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_images['set'] = 'train'\nvalidation_images['set'] = 'validation'\ntest_images['set'] = 'test'","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:25.854476Z","iopub.execute_input":"2022-02-07T14:54:25.855047Z","iopub.status.idle":"2022-02-07T14:54:25.861921Z","shell.execute_reply.started":"2022-02-07T14:54:25.855008Z","shell.execute_reply":"2022-02-07T14:54:25.860960Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"new_data = pd.concat([train_images, validation_images, test_images])\nnew_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:26.622406Z","iopub.execute_input":"2022-02-07T14:54:26.622938Z","iopub.status.idle":"2022-02-07T14:54:26.648654Z","shell.execute_reply.started":"2022-02-07T14:54:26.622903Z","shell.execute_reply":"2022-02-07T14:54:26.647968Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Train, Validation & Test Splits","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10.4, 5.4)})\nsns.countplot(x=new_data['label'], hue=new_data['set'])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:28.583099Z","iopub.execute_input":"2022-02-07T14:54:28.583703Z","iopub.status.idle":"2022-02-07T14:54:28.898818Z","shell.execute_reply.started":"2022-02-07T14:54:28.583665Z","shell.execute_reply":"2022-02-07T14:54:28.898144Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Unsampling Data","metadata":{}},{"cell_type":"code","source":"max_count = np.max(train_images.label.value_counts())\nmin_count = np.min(train_images.label.value_counts())\ntrain_images = train_images.groupby('label').sample(n=max_count, replace=True)\ntrain_images = train_images.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:30.680751Z","iopub.execute_input":"2022-02-07T14:54:30.681493Z","iopub.status.idle":"2022-02-07T14:54:30.722173Z","shell.execute_reply.started":"2022-02-07T14:54:30.681448Z","shell.execute_reply":"2022-02-07T14:54:30.721425Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_images.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:31.189179Z","iopub.execute_input":"2022-02-07T14:54:31.189703Z","iopub.status.idle":"2022-02-07T14:54:31.204621Z","shell.execute_reply.started":"2022-02-07T14:54:31.189673Z","shell.execute_reply":"2022-02-07T14:54:31.201911Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Making the Deep Learning Model","metadata":{}},{"cell_type":"code","source":"model_handle_map = {\"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\", \n                   \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4\", \n                   \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\"}\nmodel_image_size = {\"efficientnetv2-b0\": 224, \n                   \"inception_v3\": 299, \n                   \"inception_resnet_v2\": 299}","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:54:38.835364Z","iopub.execute_input":"2022-02-07T14:54:38.836056Z","iopub.status.idle":"2022-02-07T14:54:38.840574Z","shell.execute_reply.started":"2022-02-07T14:54:38.836013Z","shell.execute_reply":"2022-02-07T14:54:38.839896Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# function to decode a PNG image into a tf tensor\ndef load_image(path, label):\n    image = tf.io.read_file(path)\n    image = tf.io.decode_png(image, channels=3)\n    return image, label\n\n# reshaping the image between 0 and 1\ndef image_reshape(image, label):\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, [224, 224] / 255)\n    return image, label\n\n# image argumentation for faster model training\ndef argument_image(image):\n    transform = A.Compose([A.HorizontalFlip(p = 0.5), \n                          A.Rotate(p = 0.5, limit = 15), \n                          A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.1, 0.1), \n                                                    brightness_by_max=True),\n                           A.RandomResizedCrop(p=0.8, height=IMG_SIZE, width=IMG_SIZE, \n                                              scale=(0.9, 1.1), ratio=(0.05, 1.1), interpolation=0),\n                           A.Blur(blur_limit = (1, 1))\n                           \n                          ])\n    \n    data = {\"image\": image}\n    argumented_data = transform(**data)\n    argumented_image = argumented_data[\"image\"]\n    argumented_image = tf.cast(argumented_image, tf.float32)\n    argumented_image = tf.image.resize(argumented_image, [IMG_SIZE, IMG_SIZE]) / 255\n    \n    return argumented_image\n\ndef argumentor_function(image, label):\n    argumented_image = tf.numpy_function(func = argument_image, inp=[image], Tout = tf.float32)\n    return argumented_image, label\n\n\n# function to view sample of images\ndef view_image(ds, col = 5, row = 5, size=(30, 10)):\n    plt.figure(figsize=(10, 5))\n    plt.subplots_adjust(wspace = 0.05, hspace = 0.15)\n    for images, labels in ds.take(1):\n        for i in range(col * row):\n            ax = plt.subplot(row, col, i + 1)\n            shape = str(images[i].numpy().shape)\n            plt.imshow(images[i].numpy())\n            plt.title(class_names[labels[i].numpy()])\n            plt.axis(\"off\") \n    plt.tight_layout\n    return None\n\ndef view_model_predictions():\n    plt.figure(figsize = (30, 8))\n    plt.rcParams.update({'font.size': 10})\n    plt.subplots_adjust(wspace = 0.05, hspace = 0.15)\n    for i in range(30):\n        ax = plt.subplot(3, 10, i + 1)\n        shape = str(test_image[i].numpy().shape)\n        plt.imshow(test_image[i].numpy())\n        plt.title(predicted_label[i][0])\n        plt.axis(\"off\") \n        plt.tight_layout\n    return None\n\n# making a function to calculate & show model history\ndef model_hist(history):\n    accuracy = history['accuracy']\n    loss = history['loss']\n    val_accuracy = history['val_accuracy']\n    val_loss = history['val_loss']\n    \n    # setting the epochs\n    n_epochs = range(len(history['loss']))\n    \n    # saving models logs\n    # csv_logger = CSVLogger('cnn_model_logs.csv', append=True)\n    \n    # making plots for accuracy \n    plt.figure(figsize=(16, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(n_epochs, accuracy, label='training accuracy')\n    plt.plot(n_epochs, val_accuracy, label='validation accuracy')\n    plt.legend()\n    \n    # making plots for loss\n    plt.figure(figsize=(16, 5))\n    plt.subplot(1, 2, 2)\n    plt.plot(n_epochs, loss, label='training loss (binary crossentropy)')\n    plt.plot(n_epochs, val_loss, label='validation loss (binary crossentropy)')\n    plt.legend()\n    \n    return None\n\n# function for decoding a test image\ndef decode_test_img(path):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, [224, 224])\n    return image\n\n# function for building a NN\ndef make_nn_model(image_size):\n    print('Making our deep cnn model.....')\n    cnn_model = tf.keras.Sequential([\n        layers.InputLayer(input_shape=(image_size, image_size, 3)),\n        hub.KerasLayer(model_handle, trainable=True, name='base'),\n        layers.Dense(512, activation='relu', name='fc1'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.4, name='dropout'),\n        layers.Dense(128, activation='relu', name='fc2'),\n        layers.BatchNormalization(),\n        # layers.Dropout(0.4, name='dropout2'),\n        layers.Dense(1, activation='sigmoid', name='output')\n    ], name=model_name)\n    \n    cnn_model.build((None, image_size, image_size, 3))\n    cnn_model.summary()\n    print('model built!')\n    return cnn_model","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:24.937179Z","iopub.execute_input":"2022-02-07T14:55:24.937661Z","iopub.status.idle":"2022-02-07T14:55:24.962149Z","shell.execute_reply.started":"2022-02-07T14:55:24.937626Z","shell.execute_reply":"2022-02-07T14:55:24.961461Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Model Configuration Parameters ","metadata":{}},{"cell_type":"code","source":"# defining model configuration parameters\nmodel_name = \"efficientnetv2-b0\"\n# model_name = \"inception_v3\"\n# model_name = \"inception_resnet_v2\"\nmodel_handle = model_handle_map.get(model_name)\nIMG_SIZE = model_image_size.get(model_name, 224)\nBATCH_SIZE = 32\nEPOCHS = 10\nsample_size = len(train_images)\n\nprint(f\"Selected model: {model_name} : {model_handle}\")\nprint(f\"Input size of model: {IMG_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:31.332255Z","iopub.execute_input":"2022-02-07T14:55:31.332920Z","iopub.status.idle":"2022-02-07T14:55:31.339834Z","shell.execute_reply.started":"2022-02-07T14:55:31.332884Z","shell.execute_reply":"2022-02-07T14:55:31.338356Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:34.635543Z","iopub.execute_input":"2022-02-07T14:55:34.636077Z","iopub.status.idle":"2022-02-07T14:55:34.640747Z","shell.execute_reply.started":"2022-02-07T14:55:34.636041Z","shell.execute_reply":"2022-02-07T14:55:34.640079Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Dataset","metadata":{}},{"cell_type":"markdown","source":"We have defined the model parameters + configuration in above sections. Now we define and load the dataset in our memory. We will define 2 datasets, train & valid. ","metadata":{}},{"cell_type":"code","source":"(image_dir + train_images.path)[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:37.995487Z","iopub.execute_input":"2022-02-07T14:55:37.996331Z","iopub.status.idle":"2022-02-07T14:55:38.012571Z","shell.execute_reply.started":"2022-02-07T14:55:37.996289Z","shell.execute_reply":"2022-02-07T14:55:38.011722Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# loading the train & validation dataets\nload_train = tf.data.Dataset.from_tensor_slices((image_dir + train_images.path, \n                                                train_images.label_int))\n\nload_valid = tf.data.Dataset.from_tensor_slices((image_dir + validation_images.path, \n                                                validation_images.label_int))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:41.988291Z","iopub.execute_input":"2022-02-07T14:55:41.988633Z","iopub.status.idle":"2022-02-07T14:55:43.727711Z","shell.execute_reply.started":"2022-02-07T14:55:41.988590Z","shell.execute_reply":"2022-02-07T14:55:43.726847Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"load_train","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:48.414372Z","iopub.execute_input":"2022-02-07T14:55:48.414919Z","iopub.status.idle":"2022-02-07T14:55:48.422986Z","shell.execute_reply.started":"2022-02-07T14:55:48.414883Z","shell.execute_reply":"2022-02-07T14:55:48.422205Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n                load_train.shuffle(len(train_images))\n                .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                .map(partial(argumentor_function), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                .batch(BATCH_SIZE)\n                .prefetch(tf.data.experimental.AUTOTUNE)\n                )\nval_dataset = (\n                load_valid.shuffle(len(validation_images))\n                .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                .map(partial(argumentor_function), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                .batch(BATCH_SIZE)\n                .prefetch(tf.data.experimental.AUTOTUNE)\n                )\n\ntrain_dataset","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:55.128319Z","iopub.execute_input":"2022-02-07T14:55:55.129009Z","iopub.status.idle":"2022-02-07T14:55:55.272420Z","shell.execute_reply.started":"2022-02-07T14:55:55.128970Z","shell.execute_reply":"2022-02-07T14:55:55.271732Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# checking the path of images\ntrain_images.path[5]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:55:57.067400Z","iopub.execute_input":"2022-02-07T14:55:57.067952Z","iopub.status.idle":"2022-02-07T14:55:57.073043Z","shell.execute_reply.started":"2022-02-07T14:55:57.067916Z","shell.execute_reply":"2022-02-07T14:55:57.072372Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nview_image(train_dataset)\nend = time.time()\nprint('Time Taken: %.3f seconds' % (end-start))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:03.825136Z","iopub.execute_input":"2022-02-07T14:56:03.825865Z","iopub.status.idle":"2022-02-07T14:56:09.005134Z","shell.execute_reply.started":"2022-02-07T14:56:03.825826Z","shell.execute_reply":"2022-02-07T14:56:09.004456Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nview_image(val_dataset)\nend = time.time()\nprint('Time Taken: %.3f seconds' % (end-start))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:13.515892Z","iopub.execute_input":"2022-02-07T14:56:13.516168Z","iopub.status.idle":"2022-02-07T14:56:17.750645Z","shell.execute_reply.started":"2022-02-07T14:56:13.516134Z","shell.execute_reply":"2022-02-07T14:56:17.749914Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"We have successfully loaded the images, preprocessed them, made image argumentations and visualized the images of what they look like. ","metadata":{}},{"cell_type":"markdown","source":"## Deep Learning Model Training Using K Fold Cross Validation\nThis is the step whwre we train our deep learning model. We have defined the model configuration, dataset modelling, data loading and preparation. We will use the **K-Fold Cross Validation Technique** to train our model, by making sure that what is the best fit for our model","metadata":{}},{"cell_type":"code","source":"def get_labels_from_tfdataset(tfdataset, batched=False):\n\n    labels = list(map(lambda x: x[1], tfdataset)) # Get labels \n\n    if not batched:\n        return tf.concat(labels, axis=0) # concat the list of batched labels\n\n    return labels\nget_labels_from_tfdataset(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:56:21.182751Z","iopub.execute_input":"2022-02-07T14:56:21.183329Z","iopub.status.idle":"2022-02-07T15:03:47.965901Z","shell.execute_reply.started":"2022-02-07T14:56:21.183291Z","shell.execute_reply":"2022-02-07T15:03:47.965173Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print('Size of Image being used: %d' % (IMG_SIZE))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T15:04:58.315742Z","iopub.execute_input":"2022-02-07T15:04:58.316006Z","iopub.status.idle":"2022-02-07T15:04:58.320459Z","shell.execute_reply.started":"2022-02-07T15:04:58.315976Z","shell.execute_reply":"2022-02-07T15:04:58.319554Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"os.mkdir('model_weights')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T15:06:12.955211Z","iopub.execute_input":"2022-02-07T15:06:12.955565Z","iopub.status.idle":"2022-02-07T15:06:12.959896Z","shell.execute_reply.started":"2022-02-07T15:06:12.955530Z","shell.execute_reply":"2022-02-07T15:06:12.959179Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"%ls","metadata":{"execution":{"iopub.status.busy":"2022-02-07T15:06:56.764722Z","iopub.execute_input":"2022-02-07T15:06:56.764989Z","iopub.status.idle":"2022-02-07T15:06:57.444049Z","shell.execute_reply.started":"2022-02-07T15:06:56.764959Z","shell.execute_reply":"2022-02-07T15:06:57.443276Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# starting a new sesion for TF\nimage_size = 224\ntf.keras.backend.clear_session()\nmodel_nn = make_nn_model(IMG_SIZE)\n\n# making model checkpoints\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('model_weights/breakhis_weights_{epoch:02d}-{val_accuracy:.6f}.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True,save_weights_only=True)\n\n\n# model logs\ncsv_logger = CSVLogger('cnn_model_logs.csv', append=True)\n\nmetrics = ['accuracy', Precision(name='Precision'), Recall(name='Recall')]\n\n# compiling the model\nmodel_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)\n\n# fit the model\ntrain_history = model_nn.fit(train_dataset, epochs=10, batch_size=BATCH_SIZE, verbose=1, \n                             callbacks=[model_checkpoint, csv_logger], validation_data=val_dataset)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-07T15:07:15.021815Z","iopub.execute_input":"2022-02-07T15:07:15.022542Z","iopub.status.idle":"2022-02-07T16:45:10.107943Z","shell.execute_reply.started":"2022-02-07T15:07:15.022488Z","shell.execute_reply":"2022-02-07T16:45:10.107187Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Model Performance (Loss/Accuracy)","metadata":{}},{"cell_type":"code","source":"# visualize model performance\nhistory = train_history.history\nmodel_hist(history)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:45:18.286101Z","iopub.execute_input":"2022-02-07T16:45:18.286390Z","iopub.status.idle":"2022-02-07T16:45:18.802840Z","shell.execute_reply.started":"2022-02-07T16:45:18.286359Z","shell.execute_reply":"2022-02-07T16:45:18.802129Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"code","source":"model_performance = model_nn.evaluate(val_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:45:41.767169Z","iopub.execute_input":"2022-02-07T16:45:41.767903Z","iopub.status.idle":"2022-02-07T16:47:41.667086Z","shell.execute_reply.started":"2022-02-07T16:45:41.767867Z","shell.execute_reply":"2022-02-07T16:47:41.666307Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"print('Net loss on validation data: %.3f' % model_performance[0])\nprint('Net accuracy on validation data: %.3f' % model_performance[1])\nprint('Net precision on validation data: %.3f' % model_performance[2])\nprint('Net recall on validation data: %.3f' % model_performance[3])","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:51:40.078861Z","iopub.execute_input":"2022-02-07T16:51:40.079348Z","iopub.status.idle":"2022-02-07T16:51:40.085880Z","shell.execute_reply.started":"2022-02-07T16:51:40.079315Z","shell.execute_reply":"2022-02-07T16:51:40.085156Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation on Test Data","metadata":{}},{"cell_type":"code","source":"# making samples of test data\ntest_images = test_images.sample(frac=1).reset_index(drop=True)\ntest_data = tf.data.Dataset.from_tensor_slices(image_dir + test_images.path)\ntest_data = test_data.map(decode_test_img, \n                          num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\ntest_data","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:51:44.051231Z","iopub.execute_input":"2022-02-07T16:51:44.051717Z","iopub.status.idle":"2022-02-07T16:51:44.134768Z","shell.execute_reply.started":"2022-02-07T16:51:44.051683Z","shell.execute_reply":"2022-02-07T16:51:44.133931Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print('Total Test Images: %d' % len(test_images))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:51:47.090758Z","iopub.execute_input":"2022-02-07T16:51:47.091282Z","iopub.status.idle":"2022-02-07T16:51:47.097359Z","shell.execute_reply.started":"2022-02-07T16:51:47.091232Z","shell.execute_reply":"2022-02-07T16:51:47.096549Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Test Data Samples have been successfully created!","metadata":{}},{"cell_type":"code","source":"test_index = test_images.label_int.values\ntest_labels = test_images.label.values\ntest_index, test_labels","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:51:54.980578Z","iopub.execute_input":"2022-02-07T16:51:54.981126Z","iopub.status.idle":"2022-02-07T16:51:54.987411Z","shell.execute_reply.started":"2022-02-07T16:51:54.981089Z","shell.execute_reply":"2022-02-07T16:51:54.986500Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"test_image = next(iter(test_data))","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:51:57.851095Z","iopub.execute_input":"2022-02-07T16:51:57.851682Z","iopub.status.idle":"2022-02-07T16:51:58.546043Z","shell.execute_reply.started":"2022-02-07T16:51:57.851645Z","shell.execute_reply":"2022-02-07T16:51:58.545301Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# making predictions\ntest_predictions = model_nn.predict(test_data)\npredicted_index = np.round(test_predictions)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:52:02.261202Z","iopub.execute_input":"2022-02-07T16:52:02.261599Z","iopub.status.idle":"2022-02-07T16:52:57.337090Z","shell.execute_reply.started":"2022-02-07T16:52:02.261566Z","shell.execute_reply":"2022-02-07T16:52:57.336354Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# checking predictions made by neural network\npredicted_index[:6]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:53:41.703849Z","iopub.execute_input":"2022-02-07T16:53:41.704107Z","iopub.status.idle":"2022-02-07T16:53:41.710605Z","shell.execute_reply.started":"2022-02-07T16:53:41.704080Z","shell.execute_reply":"2022-02-07T16:53:41.709927Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"predicted_index = np.round(test_predictions).astype(int)\npredicted_label = np.array(class_names)[predicted_index]\npredicted_label[:8]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:53:44.761232Z","iopub.execute_input":"2022-02-07T16:53:44.761733Z","iopub.status.idle":"2022-02-07T16:53:44.768991Z","shell.execute_reply.started":"2022-02-07T16:53:44.761697Z","shell.execute_reply":"2022-02-07T16:53:44.768293Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"prediction_data = pd.DataFrame({'filename':test_images.filename.values,'actual':test_images.label.values, \n                              'prediction': np.squeeze(predicted_label),'path':test_images.path.values,})","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:53:47.959794Z","iopub.execute_input":"2022-02-07T16:53:47.960047Z","iopub.status.idle":"2022-02-07T16:53:47.970799Z","shell.execute_reply.started":"2022-02-07T16:53:47.960019Z","shell.execute_reply":"2022-02-07T16:53:47.970102Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# see model predictions\nprediction_data[['actual', 'prediction']].head(15)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:53:49.309771Z","iopub.execute_input":"2022-02-07T16:53:49.310466Z","iopub.status.idle":"2022-02-07T16:53:49.323889Z","shell.execute_reply.started":"2022-02-07T16:53:49.310428Z","shell.execute_reply":"2022-02-07T16:53:49.323136Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"prediction_data.to_csv('predictions.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T16:55:01.380687Z","iopub.execute_input":"2022-02-07T16:55:01.380952Z","iopub.status.idle":"2022-02-07T16:55:01.437471Z","shell.execute_reply.started":"2022-02-07T16:55:01.380923Z","shell.execute_reply":"2022-02-07T16:55:01.436785Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"prediction_data['comparison_column'] = np.where(prediction_data[\"prediction\"] == prediction_data[\"actual\"], True, False)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T17:12:31.570603Z","iopub.execute_input":"2022-02-07T17:12:31.570884Z","iopub.status.idle":"2022-02-07T17:12:31.579208Z","shell.execute_reply.started":"2022-02-07T17:12:31.570855Z","shell.execute_reply":"2022-02-07T17:12:31.578294Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"prediction_data[\"comparison_column\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T17:15:46.024592Z","iopub.execute_input":"2022-02-07T17:15:46.026425Z","iopub.status.idle":"2022-02-07T17:15:46.035967Z","shell.execute_reply.started":"2022-02-07T17:15:46.026375Z","shell.execute_reply":"2022-02-07T17:15:46.035009Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}